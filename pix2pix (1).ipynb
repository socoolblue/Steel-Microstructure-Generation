{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def lrelu(X, leak=0.2):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * X + f2 * tf.abs(X)\n",
    "\n",
    "def generator(x, ngf=64, isTrain=True, reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "        b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "        conv1 = tf.layers.conv2d(x, ngf, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        conv2 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv1, 0.2), ngf * 2, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv3 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv2, 0.2), ngf * 4, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv4 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv3, 0.2), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv5 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv4, 0.2), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv6 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv5, 0.2), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv7 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv6, 0.2), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv8 = tf.layers.conv2d(lrelu(conv7, 0.2), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "\n",
    "        deconv1 = tf.nn.dropout(tf.layers.batch_normalization(tf.layers.conv2d_transpose(tf.nn.relu(conv8), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain), keep_prob=0.5)\n",
    "        deconv1 = tf.concat([deconv1, conv7], 3)\n",
    "        deconv2 = tf.nn.dropout(tf.layers.batch_normalization(tf.layers.conv2d_transpose(tf.nn.relu(deconv1), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain), keep_prob=0.5)\n",
    "        deconv2 = tf.concat([deconv2, conv6], 3)\n",
    "        deconv3 = tf.nn.dropout(tf.layers.batch_normalization(tf.layers.conv2d_transpose(tf.nn.relu(deconv2), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain), keep_prob=0.5)\n",
    "        deconv3 = tf.concat([deconv3, conv5], 3)\n",
    "        deconv4 = tf.layers.batch_normalization(tf.layers.conv2d_transpose(tf.nn.relu(deconv3), ngf * 8, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        deconv4 = tf.concat([deconv4, conv4], 3)\n",
    "        deconv5 = tf.layers.batch_normalization(tf.layers.conv2d_transpose(tf.nn.relu(deconv4), ngf * 4, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        deconv5 = tf.concat([deconv5, conv3], 3)\n",
    "        deconv6 = tf.layers.batch_normalization(tf.layers.conv2d_transpose(tf.nn.relu(deconv5), ngf * 2, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        deconv6 = tf.concat([deconv6, conv2], 3)\n",
    "        deconv7 = tf.layers.batch_normalization(tf.layers.conv2d_transpose(tf.nn.relu(deconv6), ngf, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        deconv7 = tf.concat([deconv7, conv1], 3)\n",
    "        deconv8 = tf.layers.conv2d_transpose(tf.nn.relu(deconv7), 3, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "\n",
    "        o = tf.nn.tanh(deconv8)\n",
    "\n",
    "        return o\n",
    "\n",
    "def discriminator(x, y, ndf=64, isTrain=True, reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "\n",
    "        w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "        b_init = tf.constant_initializer(0.0)\n",
    "\n",
    "        cat1 = tf.concat([x, y], 3) \n",
    "        conv1 = tf.layers.conv2d(cat1, ndf, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "        conv2 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv1, 0.2), ndf * 2, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv3 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv2, 0.2), ndf * 4, [4, 4], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv3 = tf.pad(conv3, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
    "        conv4 = tf.layers.batch_normalization(tf.layers.conv2d(lrelu(conv3, 0.2), ndf * 8, [4, 4], strides=(1, 1), padding='valid', kernel_initializer=w_init, bias_initializer=b_init), training=isTrain)\n",
    "        conv4 = tf.pad(conv4, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
    "        conv5 = tf.layers.conv2d(lrelu(conv4, 0.2), 1, [4, 4], strides=(1, 1), padding='valid', kernel_initializer=w_init, bias_initializer=b_init)\n",
    "\n",
    "        o = tf.nn.sigmoid(conv5)\n",
    "\n",
    "        return o, conv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools, imageio, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "\n",
    "def show_result(x_, imgs, y_, num_epoch, show = False, save = False, path = 'result.png'):\n",
    "    size_figure_grid = 3\n",
    "    fig, ax = plt.subplots(x_.shape[0], size_figure_grid, figsize=(5, 5))\n",
    "    for i, j in itertools.product(range(x_.shape[0]), range(size_figure_grid)):\n",
    "        ax[i, j].get_xaxis().set_visible(False)\n",
    "        ax[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "    for i in range(x_.shape[0]):\n",
    "        ax[i, 0].cla()\n",
    "        ax[i, 0].imshow(denorm(x_[i]) / 255.0)\n",
    "        ax[i, 1].cla()\n",
    "        ax[i, 1].imshow(denorm(imgs[i]) / 255.0)\n",
    "        ax[i, 2].cla()\n",
    "        ax[i, 2].imshow(denorm(y_[i]) / 255.0)\n",
    "\n",
    "    label = 'Epoch {0}'.format(num_epoch)\n",
    "    fig.text(0.5, 0.04, label, ha='center')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "\n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def generate_animation(root, model, opt):\n",
    "    images = []\n",
    "    for e in range(opt.train_epoch):\n",
    "        img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
    "        images.append(imageio.imread(img_name))\n",
    "    imageio.mimsave(root + model + 'generate_animation.gif', images, fps=5)\n",
    "\n",
    "def imgs_resize(imgs, resize_scale = 286):\n",
    "    outputs = np.zeros((imgs.shape[0], resize_scale, resize_scale, imgs.shape[3]))\n",
    "    for i in range(imgs.shape[0]):\n",
    "        img = imresize(imgs[i], [resize_scale, resize_scale])\n",
    "        outputs[i] = img.astype(np.float32)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def random_crop(imgs1, imgs2, crop_size = 256):\n",
    "    outputs1 = np.zeros((imgs1.shape[0], crop_size, crop_size, imgs1.shape[3]))\n",
    "    outputs2 = np.zeros((imgs2.shape[0], crop_size, crop_size, imgs2.shape[3]))\n",
    "    for i in range(imgs1.shape[0]):\n",
    "        img1 = imgs1[i]\n",
    "        img2 = imgs2[i]\n",
    "        rand1 = np.random.randint(0, imgs1.shape[1] - crop_size)\n",
    "        rand2 = np.random.randint(0, imgs2.shape[1] - crop_size)\n",
    "        outputs1[i] = img1[rand1: crop_size + rand1, rand2: crop_size + rand2, :]\n",
    "        outputs2[i] = img2[rand1: crop_size + rand1, rand2: crop_size + rand2, :]\n",
    "\n",
    "    return outputs1, outputs2\n",
    "\n",
    "def random_fliplr(imgs1, imgs2):\n",
    "    outputs1 = np.zeros(imgs1.shape)\n",
    "    outputs2 = np.zeros(imgs2.shape)\n",
    "    for i in range(imgs1.shape[0]):\n",
    "        if np.random.rand(1) < 0.5:\n",
    "            outputs1[i] = np.fliplr(imgs1[i])\n",
    "            outputs2[i] = np.fliplr(imgs2[i])\n",
    "        else:\n",
    "            outputs1[i] = imgs1[i]\n",
    "            outputs2[i] = imgs2[i]\n",
    "\n",
    "    return outputs1, outputs2\n",
    "\n",
    "def norm(img):\n",
    "    return (img - 127.5) / 127.5\n",
    "\n",
    "def denorm(img):\n",
    "    return (img * 127.5) + 127.5\n",
    "\n",
    "class data_loader:\n",
    "    def __init__(self, root, batch_size=1, shuffle=False):\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.file_list = os.listdir(self.root)\n",
    "        if shuffle:\n",
    "            self.file_list = list(np.array(self.file_list)[random.sample(range(0, len(self.file_list)), len(self.file_list))])\n",
    "        img = plt.imread(self.root + '/' + self.file_list[0])\n",
    "        self.shape = (len(self.file_list), img.shape[0], img.shape[1], img.shape[2])\n",
    "        self.flag = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.flag + self.batch_size > self.shape[0]:\n",
    "            self.file_list = list(np.array(self.file_list)[random.sample(range(0, len(self.file_list)), len(self.file_list))])\n",
    "            self.flag = 0\n",
    "\n",
    "        output = np.zeros((self.batch_size, self.shape[1], self.shape[2], self.shape[3]))\n",
    "        temp = 0\n",
    "        for i in range(self.flag, self.flag + self.batch_size):\n",
    "            output[temp] = plt.imread(self.root + '/' + self.file_list[i])\n",
    "            temp = temp + 1\n",
    "\n",
    "        self.flag += self.batch_size\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle, easydict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "opt= easydict.EasyDict({ \"dataset\": 'steels', \n",
    "                        \"train_subfolder\": 'train',\n",
    "                        \"test_subfolder\": 'val',\n",
    "                        \"batch_size\":1,\n",
    "                        \"test_batch_size\": 2,\n",
    "                        \"ngf\":64,\n",
    "                        \"ndf\":64,\n",
    "                        \"input_size\":256,\n",
    "                        \"crop_size\":256,\n",
    "                        \"resize_scale\":286,\n",
    "                        \"fliplr\":True,\n",
    "                        \"train_epoch\":200,\n",
    "                        \"lrD\":0.0002,\n",
    "                        \"lrG\":0.0002,\n",
    "                        \"L1_lambda\":100,\n",
    "                        \"beta1\":0.5,\n",
    "                        \"beta2\":0.999,\n",
    "                        \"save_root\":'results',\n",
    "                        \"inverse_order\":True\n",
    "                       })\n",
    "\n",
    "root = opt.dataset + '_' + opt.save_root + '/'\n",
    "model = opt.dataset + '_'\n",
    "if not os.path.isdir(root):\n",
    "    os.mkdir(root)\n",
    "if not os.path.isdir(root + 'Fixed_results'):\n",
    "    os.mkdir(root + 'Fixed_results')\n",
    "\n",
    "train_loader = data_loader('D:/SEM and OM pix2pix data/data/' + opt.dataset + '/' + opt.train_subfolder, opt.batch_size, shuffle=True)\n",
    "test_loader = data_loader('D:/SEM and OM pix2pix data/data/' + opt.dataset + '/' + opt.test_subfolder, opt.test_batch_size, shuffle=True)\n",
    "img_size = train_loader.shape[1]\n",
    "test_img = test_loader.next_batch()\n",
    "if opt.inverse_order:\n",
    "    fixed_x_ = test_img[:, :, img_size:, :]\n",
    "    fixed_y_ = test_img[:, :, 0:img_size, :]\n",
    "else:\n",
    "    fixed_x_ = test_img[:, :, 0:img_size, :]\n",
    "    fixed_y_ = test_img[:, :, img_size:, :]\n",
    "\n",
    "if img_size != opt.input_size:\n",
    "    fixed_x_ = imgs_resize(fixed_x_, opt.input_size)\n",
    "    fixed_y_ = imgs_resize(fixed_y_, opt.input_size)\n",
    "\n",
    "fixed_x_ = norm(fixed_x_)\n",
    "fixed_y_ = norm(fixed_y_)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, opt.input_size, opt.input_size, train_loader.shape[3]))\n",
    "y = tf.placeholder(tf.float32, shape=(None, opt.input_size, opt.input_size, train_loader.shape[3]))\n",
    "\n",
    "G = generator(x, opt.ngf)\n",
    "D_positive, D_positive_logits = discriminator(x, y, opt.ndf)\n",
    "D_negative, D_negative_logits = discriminator(x, G, opt.ndf, reuse=True)\n",
    "\n",
    "D_loss_positive = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_positive_logits, labels=tf.ones_like(D_positive_logits)))\n",
    "D_loss_negative = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_negative_logits, labels=tf.zeros_like(D_negative_logits)))\n",
    "D_loss = (D_loss_positive + D_loss_negative) * 0.5\n",
    "G_loss_gan = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_negative_logits, labels=tf.ones_like(D_negative_logits)))\n",
    "G_loss_L1 = tf.reduce_mean(tf.reduce_sum(tf.abs(G - y), 3))\n",
    "G_loss = G_loss_gan + opt.L1_lambda * G_loss_L1\n",
    "\n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
    "\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    D_optim = tf.train.AdamOptimizer(opt.lrD, beta1=opt.beta1, beta2=opt.beta2).minimize(D_loss, var_list=D_vars)\n",
    "    G_optim = tf.train.AdamOptimizer(opt.lrG, beta1=opt.beta1, beta2=opt.beta2).minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "train_hist = {}\n",
    "train_hist['D_losses'] = []\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['per_epoch_ptimes'] = []\n",
    "train_hist['total_ptime'] = []\n",
    "\n",
    "print('training start!')\n",
    "start_time = time.time()\n",
    "for epoch in range(opt.train_epoch):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    epoch_start_time = time.time()\n",
    "    num_iter = 0\n",
    "    for iter in range(train_loader.shape[0] // opt.batch_size):\n",
    "        train_img = train_loader.next_batch()\n",
    "\n",
    "        if opt.inverse_order:\n",
    "            x_ = train_img[:, :, img_size:, :]\n",
    "            y_ = train_img[:, :, 0:img_size, :]\n",
    "        else:\n",
    "            x_ = train_img[:, :, 0:img_size, :]\n",
    "            y_ = train_img[:, :, img_size:, :]\n",
    "\n",
    "        if img_size != opt.input_size:\n",
    "            x_ = imgs_resize(x_, opt.input_size)\n",
    "            y_ = imgs_resize(y_, opt.input_size)\n",
    "\n",
    "        if opt.resize_scale:\n",
    "            x_ = imgs_resize(x_, opt.resize_scale)\n",
    "            y_ = imgs_resize(y_, opt.resize_scale)\n",
    "\n",
    "        if opt.crop_size:\n",
    "            x_, y_ = random_crop(x_, y_, opt.crop_size)\n",
    "\n",
    "        if opt.fliplr:\n",
    "            x_, y_ = random_fliplr(x_, y_)\n",
    "\n",
    "        x_ = norm(x_)\n",
    "        y_ = norm(y_)\n",
    "\n",
    "        loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, y: y_})\n",
    "        D_losses.append(loss_d_)\n",
    "        train_hist['D_losses'].append(loss_d_)\n",
    "\n",
    "        loss_g_, _ = sess.run([G_loss, G_optim], {x: x_, y: y_})\n",
    "        G_losses.append(loss_g_)\n",
    "        train_hist['G_losses'].append(loss_g_)\n",
    "\n",
    "        num_iter += 1\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "\n",
    "    print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), opt.train_epoch, per_epoch_ptime, np.mean((D_losses)), np.mean(G_losses)))\n",
    "    fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n",
    "    outputs = sess.run(G, {x: fixed_x_})\n",
    "    show_result(fixed_x_, outputs, fixed_y_, (epoch+1), save=True, path=fixed_p)\n",
    "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "\n",
    "end_time = time.time()\n",
    "total_ptime = end_time - start_time\n",
    "train_hist['total_ptime'].append(total_ptime)\n",
    "\n",
    "print(\"Avg. one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (np.mean(train_hist['per_epoch_ptimes']), opt.train_epoch, total_ptime))\n",
    "print(\"Training finish!... save training results\")\n",
    "with open(root + model + 'train_hist.pkl', 'wb') as f:\n",
    "    pickle.dump(train_hist, f)\n",
    "\n",
    "saver.save(sess, root + model + 'params.ckpt')\n",
    "\n",
    "show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')\n",
    "\n",
    "#pix2pix_training_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, easydict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "opt= easydict.EasyDict({ \"dataset\": 'steels',\n",
    "                        \"input_size\":256,\n",
    "                        \"test_subfolder\": 'val',\n",
    "                        \"ngf\":64,\n",
    "                        \"ndf\":64,\n",
    "                        \"save_root\":'results',\n",
    "                        \"inverse_order\":True\n",
    "                       })\n",
    "\n",
    "if not os.path.isdir(opt.dataset + '_results/test_results'):\n",
    "    os.mkdir(opt.dataset + '_results/test_results')\n",
    "\n",
    "test_loader = data_loader('D:/SEM and OM pix2pix data/data/' + opt.dataset + '/' + opt.test_subfolder, 1, shuffle=False)\n",
    "img_size = test_loader.shape[1]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, opt.input_size, opt.input_size, test_loader.shape[3]))\n",
    "\n",
    "G = generator(x, opt.ngf)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "saver.restore(sess, (opt.dataset + '_results/steels_params.ckpt'))\n",
    "\n",
    "print('test start!')\n",
    "\n",
    "per_ptime = []\n",
    "total_start_time = time.time()\n",
    "for iter in range(test_loader.shape[0]):\n",
    "    per_start_time = time.time()\n",
    "\n",
    "    train_img = test_loader.next_batch()\n",
    "\n",
    "    if opt.inverse_order:\n",
    "        x_ = train_img[:, :, img_size:, :]\n",
    "        y_ = train_img[:, :, 0:img_size, :]\n",
    "    else:\n",
    "        x_ = train_img[:, :, 0:img_size, :]\n",
    "        y_ = train_img[:, :, img_size:, :]\n",
    "\n",
    "    if img_size != opt.input_size:\n",
    "        x_ = imgs_resize(x_, opt.input_size)\n",
    "        y_ = imgs_resize(y_, opt.input_size)\n",
    "\n",
    "    x_ = norm(x_)\n",
    "    y_ = norm(y_)\n",
    "\n",
    "    test_img = sess.run(G, {x: x_})\n",
    "\n",
    "    num_str = test_loader.file_list[iter][:test_loader.file_list[iter].find('.')]\n",
    "    path = opt.dataset + '_results/test_results/' + num_str + '_input.png'\n",
    "    plt.imsave(path, (denorm(x_[0]) / 255))\n",
    "    path = opt.dataset + '_results/test_results/' + num_str + '_output.png'\n",
    "    plt.imsave(path, (denorm(test_img[0]) / 255))\n",
    "    path = opt.dataset + '_results/test_results/' + num_str + '_target.png'\n",
    "\n",
    "    per_end_time = time.time()\n",
    "    per_ptime.append(per_end_time - per_start_time)\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_ptime = total_end_time - total_start_time\n",
    "\n",
    "print('total %d images generation complete!' % (iter+1))\n",
    "print('Avg. one image process ptime: %.2f, total %d images process ptime: %.2f' % (np.mean(per_ptime), (iter+1), total_ptime))\n",
    "\n",
    "#pix2pix_test_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
