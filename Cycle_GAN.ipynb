{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PIL import ImageFilter\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "h5y_r = h5py.File('intput_path', 'r')\n",
    "X_data = h5y_r['OM_x'][:]/255\n",
    "print(X_data.shape)\n",
    "h5y_r.close()\n",
    "\n",
    "h5y_r = h5py.File('intput_path', 'r')\n",
    "Y_data = h5y_r['SEM_y'][:]/255\n",
    "print(Y_data.shape)\n",
    "h5y_r.close()\n",
    "\n",
    "#%%\n",
    "#index separation\n",
    "tot_ix =range(len(X_data))\n",
    "test_ix=np.random.choice(tot_ix, round(len(X_data)*0.2))\n",
    "\n",
    "mat1 = test_ix\n",
    "dataframe1 = pd.DataFrame(data=mat1.astype(int))\n",
    "dataframe1.to_csv('intput_path', sep=',', header=False, index=False)\n",
    "\n",
    "train_ix = list(set(tot_ix) - set(test_ix))\n",
    "#%%\n",
    "#data separate\n",
    "\n",
    "test_ix = pd.read_csv('intput_path',header=None)\n",
    "test_ix= test_ix[0].tolist()\n",
    "tot_ix =range(len(X_data))\n",
    "train_ix = list(set(tot_ix) - set(test_ix))\n",
    "\n",
    "train_X=X_data[train_ix]\n",
    "train_Y=Y_data[train_ix]\n",
    "test_X=X_data[test_ix]\n",
    "test_Y=Y_data[test_ix]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_Y))\n",
    "\n",
    "#%%\n",
    "runfile('intput_path', wdir='D://cyclegan')\n",
    "\n",
    "n_epoch = 3000\n",
    "batch_size = 5\n",
    "\n",
    "#import tqdm\n",
    "losses = {\"d_X\":[], \"d_Y\":[], \"gan_XY\":[]}\n",
    "\n",
    "for i in range(n_epoch) :\n",
    "    batch1 =  np.random.choice(range(len(train_X)), batch_size)\n",
    "    batch2 =  np.random.choice(range(len(train_Y)), batch_size)\n",
    "    X_ = train_X[batch1]\n",
    "    Y_ = train_Y[batch2]\n",
    "    \n",
    "    Label = np.zeros([2*batch_size,15,15,1])\n",
    "    Label[0:batch_size] = 1\n",
    "    Label[batch_size:] = 0\n",
    "    \n",
    "    G_X_image = G_model.predict(X_)\n",
    "    Y_ = np.concatenate((Y_, G_X_image))\n",
    "\n",
    "    d_Y_loss = D_Y_model.train_on_batch(Y_, Label)\n",
    "    losses[\"d_Y\"].append(d_Y_loss)\n",
    "    \n",
    "    F_Y_image = F_model.predict(Y_[0:batch_size])\n",
    "    X_ = np.concatenate((X_, F_Y_image))\n",
    "\n",
    "    d_X_loss = D_X_model.train_on_batch(X_, Label)\n",
    "    losses[\"d_X\"].append(d_X_loss)\n",
    "        \n",
    "    X_ = train_X[batch1]\n",
    "    Y_ = train_Y[batch2]\n",
    "    \n",
    "    Label = Label[0:batch_size]\n",
    "    \n",
    "    gan_XY_loss = GAN_XY_model.train_on_batch([X_,Y_],[X_, Y_, Label, Label])\n",
    "    losses[\"gan_XY\"].append(gan_XY_loss)\n",
    "    \n",
    "    if i%10 == 0: \n",
    "        print(i, d_X_loss, d_Y_loss, gan_XY_loss[4])\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(np.mean(D_X_model.predict(X_)), np.mean(D_X_model.predict(F_Y_image)))\n",
    "        print(np.mean(D_Y_model.predict(Y_)), np.mean(D_Y_model.predict(G_X_image)))\n",
    "        plt.imshow(np.squeeze(np.uint8(F_model.predict(test_Y)[10]*255)))\n",
    "        plt.show()\n",
    "\n",
    "G_predicted = G_model.predict(test_X)*255\n",
    "F_predicted = F_model.predict(test_Y)*255\n",
    "F_model.save_weights('save_path'+str(2300)+'.out')\n",
    "G_model.save_weights('save_path'+str(2300)+'.out')\n",
    "\n",
    "t1 = np.squeeze(np.uint8(train_Y[1]*255)); t2 = np.squeeze(np.uint8(F_model.predict(train_Y)[1]*255))\n",
    "t3 = np.squeeze(np.uint8(G_model.predict(F_model.predict(train_Y))[1]*255))\n",
    "plt.imshow(t1);plt.show();plt.imshow(t2);plt.show();plt.imshow(t3);plt.show()\n",
    "\n",
    "t1 = np.squeeze(np.uint8(train_X[1]*255)); t2 = np.squeeze(np.uint8(F_model.predict(train_X)[1]*255))\n",
    "t3 = np.squeeze(np.uint8(G_model.predict(F_model.predict(train_X))[1]*255))\n",
    "plt.imshow(t1);plt.show();plt.imshow(t2);plt.show();plt.imshow(t3);plt.show()\n",
    "\n",
    "for i in range(100):\n",
    "    a128=np.uint8(G_predicted[i])\n",
    "    a128=np.squeeze(a128)\n",
    "    test_img=Image.fromarray(a128)\n",
    "    test_img.save(\"save_path\"+str(i)+\".png\")\n",
    "\n",
    "for i in range(100):\n",
    "    a128=np.uint8(F_predicted[i])\n",
    "    a128=np.squeeze(a128)\n",
    "    test_img=Image.fromarray(a128)\n",
    "    test_img.save(\"save_path\"+str(i)+\".png\")\n",
    "\n",
    "for i in range(100):\n",
    "    a128=np.uint8(test_X[i]*255)\n",
    "    a128=np.squeeze(a128)\n",
    "    test_img=Image.fromarray(a128)\n",
    "    test_img.save(\"save_path\"+str(i)+\".png\")      \n",
    "    \n",
    "for i in range(100):\n",
    "    a128=np.uint8(test_Y[i]*255)\n",
    "    a128=np.squeeze(a128)\n",
    "    test_img=Image.fromarray(a128)\n",
    "    test_img.save(\"save_path\"+str(i)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Merge, Concatenate ,Merge , Add\n",
    "from keras.layers import Dropout, Activation, LeakyReLU, PReLU, ELU, ThresholdedReLU\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, AveragePooling1D, AveragePooling2D, ZeroPadding2D\n",
    "from keras.layers import ZeroPadding2D, Conv2DTranspose, UpSampling2D, Lambda\n",
    "from keras.layers import Input, Dense, TimeDistributed, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.engine.topology import Container\n",
    "#from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LSTM, Reshape\n",
    "import keras.callbacks\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "\n",
    "opt = Adam(lr=1e-4)\n",
    "dopt = Adam(lr=1e-5)\n",
    "cycle_loss_weight = 8\n",
    "flag = 'mix2'\n",
    "\n",
    "def discriminator_model():\n",
    "    \n",
    "    x_input = Input(shape=(256,256,1))\n",
    "    \n",
    "    x = ZeroPadding2D(padding=(1, 1))(x_input)\n",
    "    x = Conv2D(64, (4, 4), strides=(2, 2), padding='valid', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    \n",
    "    x = Conv2D(2*64, (4, 4), strides=(2, 2), padding='valid', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "    #x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    \n",
    "    x = Conv2D(4*64, (4, 4), strides=(2, 2), padding='valid', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "    #x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    \n",
    "    x = Conv2D(6*64, (4, 4), strides=(2, 2), padding='valid', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "    #x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    \n",
    "\n",
    "    x = Conv2D(1, (4, 4), activation='sigmoid', strides=(1, 1))(x)\n",
    "    \n",
    "    #x = Flatten()(x)\n",
    "    #x = Dense(2,activation='softmax')(x)\n",
    "    \n",
    "    final_model = Model(inputs = x_input, outputs = x)\n",
    "    final_model.compile(optimizer = dopt, loss = 'mse')\n",
    "    \n",
    "    return final_model \n",
    "\n",
    "def generator_model(flag):\n",
    "    \n",
    "    x_input = Input(shape=(256,256,1))\n",
    "    #CONV block\n",
    "    x = Conv2D(128, (7, 7), strides=(1, 1), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x_input)\n",
    "    x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "    x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "    x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    for i in range(6):\n",
    "        x = residual_block(x)\n",
    "\n",
    "    if flag == 'deconv':\n",
    "        x = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "    if flag == 'upsample' :\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "    if flag == 'mix' :\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = 'same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "    if flag == 'mix1' :\n",
    "        x = Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = 'same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(x)\n",
    "        x = Activation('relu')(x)\n",
    "    if flag == 'mix2' :\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(256, (2, 2), strides = (1, 1), padding = 'same', kernel_initializer = RandomNormal(0, 0.03))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.03))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same', kernel_initializer = RandomNormal(0, 0.03))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.03))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(128, (2, 2), strides = (1, 1), padding = 'same', kernel_initializer = RandomNormal(0, 0.03))(x)\n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.03))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(128, (2, 2), strides = (1, 1), padding = 'same', kernel_initializer = RandomNormal(0, 0.03))(x) \n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.03))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(64, (2, 2), strides = (1, 1), padding = 'same', kernel_initializer = RandomNormal(0, 0.03))(x) \n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.03))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2D(64, (2, 2), strides = (1, 1), padding = 'same', kernel_initializer = RandomNormal(0, 0.03))(x) \n",
    "        x = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.03))(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "    conv_outputs = Conv2D(1, (7, 7), activation='tanh', strides=(1, 1) ,padding='same')(x)  \n",
    "    \n",
    "    final_model = Model(inputs = x_input, outputs = conv_outputs)\n",
    "    \n",
    "    return final_model, x_input, conv_outputs\n",
    "\n",
    "def residual_block(x):\n",
    "    y = Conv2D(256, (3, 3), strides=(1, 1), padding='same', kernel_initializer = RandomNormal(0, 0.02))(x)\n",
    "    y = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(y)\n",
    "    y = LeakyReLU(alpha=0.2)(y)\n",
    "    y = Conv2D(256, (3, 3), strides=(1, 1), padding='same', kernel_initializer = RandomNormal(0, 0.02))(y)\n",
    "    y = BatchNormalization(momentum=0.9, axis=3, epsilon=1e-5, gamma_initializer = RandomNormal(1., 0.02))(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return Add()([y, x])\n",
    "\n",
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "\n",
    "D_X_model = discriminator_model()\n",
    "D_Y_model = discriminator_model()\n",
    "\n",
    "G_model, realX, fakeY = generator_model(flag)\n",
    "F_model, realY, fakeX = generator_model(flag)\n",
    "\n",
    "make_trainable(D_X_model, False)\n",
    "make_trainable(D_Y_model, False)\n",
    "\n",
    "GAN_XY_model = Model(inputs = [realX, realY], \n",
    "                     outputs = [  F_model(fakeY), G_model(fakeX), D_X_model(fakeX),  D_Y_model(fakeY)])\n",
    "\n",
    "GAN_XY_model.compile(optimizer=opt, loss=['mae','mae','mse','mse'], \n",
    "                     loss_weights=[cycle_loss_weight, cycle_loss_weight, 1., 1.])\n",
    "\n",
    "print(len(D_Y_model.trainable_weights), len(D_Y_model._collected_trainable_weights))\n",
    "print(len(GAN_XY_model.trainable_weights),len(GAN_XY_model._collected_trainable_weights))\n",
    "\n",
    "GAN_XY_model.summary()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
